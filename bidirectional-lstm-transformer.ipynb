{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12406681,"sourceType":"datasetVersion","datasetId":7823994}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"284e5079","cell_type":"markdown","source":"# DNA Sequence Classification ‚Äî Bidirectional LSTM + Transformer\nThis notebook demonstrates a DNA sequence classification model that combines Bidirectional LSTM and Transformer encoder architecture.\n\n**Steps included:**\n- Data loading and preprocessing\n- Model architecture (BiLSTM + Transformer)\n- Training and evaluation\n- Example predictions","metadata":{}},{"id":"4eb667eb-28d5-457f-93cf-100b6767b455","cell_type":"code","source":"# =========================================================\n# üß¨ DNA Sequence Classification ‚Äì BiLSTM + Transformer\n# =========================================================\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, Embedding, Bidirectional, LSTM, Dense,\n    Dropout, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport joblib\n\n# ------------------------------------------------------\n# üß© 1. Data Augmentation Utilities\n# ------------------------------------------------------\ndef augment_sequence(seq, n=3):\n    seqs = []\n    for _ in range(n):\n        shuffled = ''.join(np.random.permutation(list(seq)))\n        seqs.append(shuffled)\n    return seqs\n\ndef expand_small_classes(df, min_size=10):\n    dfs = []\n    for label, sub in df.groupby(\"GeneType\"):\n        if len(sub) < min_size:\n            needed = min_size - len(sub)\n            aug = pd.DataFrame({\n                \"Sequence\": sum([augment_sequence(s, n=needed//len(sub)+1) for s in sub[\"Sequence\"]], [])[:needed],\n                \"GeneType\": label\n            })\n            dfs.append(pd.concat([sub, aug]))\n        else:\n            dfs.append(sub)\n    return pd.concat(dfs).reset_index(drop=True)\n\n# ------------------------------------------------------\n# üß† 2. Model Builder ‚Äì BiLSTM + Transformer Block\n# ------------------------------------------------------\ndef build_model(vocab_size, num_classes, max_len=200):\n    inputs = Input(shape=(max_len,))\n    x = Embedding(vocab_size, 64)(inputs)\n    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n    attn = MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\n    x = LayerNormalization(epsilon=1e-6)(x + attn)\n    x = GlobalAveragePooling1D()(x)\n    x = Dropout(0.3)(x)\n    \n    if num_classes == 2:\n        outputs = Dense(1, activation='sigmoid')(x)\n        loss = 'binary_crossentropy'\n    else:\n        outputs = Dense(num_classes, activation='softmax')(x)\n        loss = 'sparse_categorical_crossentropy'\n    \n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(1e-4), loss=loss, metrics=['accuracy'])\n    return model\n\n# ------------------------------------------------------\n# üöÄ 3. Training Function\n# ------------------------------------------------------\ndef train_model(df, task):\n    print(f\"\\n===== Training {task} =====\")\n    \n    # Expand small classes for stable training\n    df = expand_small_classes(df, min_size=20).sample(frac=1, random_state=42).reset_index(drop=True)\n    df = df.copy()\n    \n    # Re-map classes based on task\n    if task == 'model2':\n        df['GeneType'] = df['GeneType'].apply(lambda x: 'PSEUDO' if x == 'PSEUDO' else 'OTHERS')\n    elif task == 'model3':\n        df['GeneType'] = df['GeneType'].apply(lambda x: x if x in ['PSEUDO', 'BIOLOGICAL_REGION'] else 'OTHERS')\n    elif task == 'model4':\n        df['GeneType'] = df['GeneType'].apply(lambda x: x if x in ['PSEUDO','BIOLOGICAL_REGION','ncRNA'] else 'OTHERS')\n\n    # Encode labels\n    le = LabelEncoder()\n    df['label'] = le.fit_transform(df['GeneType'])\n\n    # Split safely\n    stratify = df['label'] if df['label'].value_counts().min() >= 2 else None\n    X_train, X_temp, y_train, y_temp = train_test_split(df['Sequence'], df['label'], test_size=0.3, random_state=42, stratify=stratify)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n    # Tokenize\n    tokenizer = Tokenizer(char_level=True)\n    tokenizer.fit_on_texts(X_train)\n    max_len = 200\n    X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len, padding='post')\n    X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=max_len, padding='post')\n    X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding='post')\n\n    vocab_size = len(tokenizer.word_index) + 1\n    num_classes = len(le.classes_)\n\n    # Build & Train\n    model = build_model(vocab_size, num_classes)\n    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    model.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val),\n              epochs=15, batch_size=16, callbacks=[es], verbose=2)\n    \n    # Evaluate\n    loss, acc = model.evaluate(X_test_seq, y_test, verbose=0)\n    print(f\"‚úÖ {task} Test Accuracy: {acc:.4f}\")\n\n    # Save\n    model.save(f\"{task}_bilstm_transformer.keras\")\n    joblib.dump(tokenizer, f\"{task}_tokenizer.pkl\")\n    joblib.dump(le, f\"{task}_le.pkl\")\n\n    return model, tokenizer, le\n\n# ------------------------------------------------------\n# üîÆ 4. Prediction Function\n# ------------------------------------------------------\ndef predict_sequence(sequence, model_path, tokenizer_path, le_path, max_len=200):\n    model = load_model(model_path)\n    tokenizer = joblib.load(tokenizer_path)\n    le = joblib.load(le_path)\n\n    seq = pad_sequences(tokenizer.texts_to_sequences([sequence]), maxlen=max_len, padding='post')\n    pred = model.predict(seq)\n    \n    if pred.shape[1] == 1:  # Binary\n        predicted = int(pred[0][0] > 0.5)\n    else:  # Multiclass\n        predicted = np.argmax(pred, axis=1)[0]\n    \n    gene_type = le.inverse_transform([predicted])[0]\n    print(f\"üî¨ Predicted GeneType: {gene_type}\")\n    return gene_type\n\n# ------------------------------------------------------\n# üß™ 5. Example Run (Sample Data)\n# ------------------------------------------------------\ndf = pd.DataFrame({\n    \"Sequence\": [\n        \"ATGCGTACGATCG\", \"TTTTTACGCGCGT\", \"GCGCGTATATATA\", \n        \"CGTATGCGTACGT\", \"AACCGGTT\", \"GCGCGCGCGC\", \"ATATATATAT\"\n    ],\n    \"GeneType\": [\n        \"PSEUDO\", \"BIOLOGICAL_REGION\", \"ncRNA\", \"OTHER_TYPE\", \"PSEUDO\", \"BIOLOGICAL_REGION\", \"OTHER_TYPE\"\n    ]\n})\n\n# Train models\nmodel1, tokenizer1, le1 = train_model(df, 'model1')\nmodel2, tokenizer2, le2 = train_model(df, 'model2')\nmodel3, tokenizer3, le3 = train_model(df, 'model3')\nmodel4, tokenizer4, le4 = train_model(df, 'model4')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:28:38.130167Z","iopub.execute_input":"2025-10-26T14:28:38.130516Z","iopub.status.idle":"2025-10-26T14:29:28.903594Z","shell.execute_reply.started":"2025-10-26T14:28:38.130492Z","shell.execute_reply":"2025-10-26T14:29:28.903013Z"}},"outputs":[{"name":"stdout","text":"\n===== Training model1 =====\nEpoch 1/15\n4/4 - 7s - 2s/step - accuracy: 0.2143 - loss: 1.9154 - val_accuracy: 0.2500 - val_loss: 1.4717\nEpoch 2/15\n4/4 - 1s - 127ms/step - accuracy: 0.2857 - loss: 1.7133 - val_accuracy: 0.1667 - val_loss: 1.7154\nEpoch 3/15\n4/4 - 1s - 126ms/step - accuracy: 0.2679 - loss: 1.7554 - val_accuracy: 0.1667 - val_loss: 1.4616\nEpoch 4/15\n4/4 - 0s - 123ms/step - accuracy: 0.3393 - loss: 1.6014 - val_accuracy: 0.3333 - val_loss: 1.3836\nEpoch 5/15\n4/4 - 0s - 110ms/step - accuracy: 0.3393 - loss: 1.7185 - val_accuracy: 0.3333 - val_loss: 1.3803\nEpoch 6/15\n4/4 - 0s - 110ms/step - accuracy: 0.2857 - loss: 1.6633 - val_accuracy: 0.3333 - val_loss: 1.3637\nEpoch 7/15\n4/4 - 0s - 104ms/step - accuracy: 0.3036 - loss: 1.5942 - val_accuracy: 0.2500 - val_loss: 1.3964\nEpoch 8/15\n4/4 - 0s - 109ms/step - accuracy: 0.2321 - loss: 1.6520 - val_accuracy: 0.2500 - val_loss: 1.4322\nEpoch 9/15\n4/4 - 0s - 104ms/step - accuracy: 0.3750 - loss: 1.4974 - val_accuracy: 0.2500 - val_loss: 1.4032\nEpoch 10/15\n4/4 - 0s - 97ms/step - accuracy: 0.1964 - loss: 1.7303 - val_accuracy: 0.2500 - val_loss: 1.3755\nEpoch 11/15\n4/4 - 0s - 117ms/step - accuracy: 0.3929 - loss: 1.5007 - val_accuracy: 0.2500 - val_loss: 1.3810\n‚úÖ model1 Test Accuracy: 0.1667\n\n===== Training model2 =====\nEpoch 1/15\n4/4 - 6s - 2s/step - accuracy: 0.4643 - loss: 0.9408 - val_accuracy: 0.8333 - val_loss: 0.4910\nEpoch 2/15\n4/4 - 0s - 109ms/step - accuracy: 0.7321 - loss: 0.6423 - val_accuracy: 0.8333 - val_loss: 0.4965\nEpoch 3/15\n4/4 - 0s - 110ms/step - accuracy: 0.7321 - loss: 0.6859 - val_accuracy: 0.8333 - val_loss: 0.4510\nEpoch 4/15\n4/4 - 0s - 100ms/step - accuracy: 0.7143 - loss: 0.5714 - val_accuracy: 0.8333 - val_loss: 0.4729\nEpoch 5/15\n4/4 - 0s - 101ms/step - accuracy: 0.6250 - loss: 0.8037 - val_accuracy: 0.8333 - val_loss: 0.5011\nEpoch 6/15\n4/4 - 0s - 106ms/step - accuracy: 0.6964 - loss: 0.6600 - val_accuracy: 0.8333 - val_loss: 0.4868\nEpoch 7/15\n4/4 - 0s - 105ms/step - accuracy: 0.6607 - loss: 0.6858 - val_accuracy: 0.8333 - val_loss: 0.4574\nEpoch 8/15\n4/4 - 0s - 112ms/step - accuracy: 0.7500 - loss: 0.5555 - val_accuracy: 0.8333 - val_loss: 0.4504\nEpoch 9/15\n4/4 - 0s - 104ms/step - accuracy: 0.7143 - loss: 0.6781 - val_accuracy: 0.8333 - val_loss: 0.4504\nEpoch 10/15\n4/4 - 1s - 161ms/step - accuracy: 0.7500 - loss: 0.6174 - val_accuracy: 0.8333 - val_loss: 0.4587\nEpoch 11/15\n4/4 - 0s - 112ms/step - accuracy: 0.7143 - loss: 0.5836 - val_accuracy: 0.8333 - val_loss: 0.4575\nEpoch 12/15\n4/4 - 0s - 113ms/step - accuracy: 0.7321 - loss: 0.6077 - val_accuracy: 0.8333 - val_loss: 0.4498\nEpoch 13/15\n4/4 - 0s - 105ms/step - accuracy: 0.7500 - loss: 0.5572 - val_accuracy: 0.8333 - val_loss: 0.4514\nEpoch 14/15\n4/4 - 1s - 151ms/step - accuracy: 0.7500 - loss: 0.5327 - val_accuracy: 0.8333 - val_loss: 0.4604\nEpoch 15/15\n4/4 - 0s - 105ms/step - accuracy: 0.7143 - loss: 0.6784 - val_accuracy: 0.8333 - val_loss: 0.4544\n‚úÖ model2 Test Accuracy: 0.6667\n\n===== Training model3 =====\nEpoch 1/15\n4/4 - 6s - 2s/step - accuracy: 0.3036 - loss: 1.4371 - val_accuracy: 0.4167 - val_loss: 1.1665\nEpoch 2/15\n4/4 - 0s - 109ms/step - accuracy: 0.4286 - loss: 1.1622 - val_accuracy: 0.4167 - val_loss: 1.2323\nEpoch 3/15\n4/4 - 0s - 106ms/step - accuracy: 0.4643 - loss: 1.1437 - val_accuracy: 0.4167 - val_loss: 1.2135\nEpoch 4/15\n4/4 - 0s - 110ms/step - accuracy: 0.4286 - loss: 1.3951 - val_accuracy: 0.4167 - val_loss: 1.0832\nEpoch 5/15\n4/4 - 0s - 109ms/step - accuracy: 0.4464 - loss: 1.2508 - val_accuracy: 0.4167 - val_loss: 1.0800\nEpoch 6/15\n4/4 - 0s - 104ms/step - accuracy: 0.3036 - loss: 1.3448 - val_accuracy: 0.4167 - val_loss: 1.1051\nEpoch 7/15\n4/4 - 0s - 103ms/step - accuracy: 0.3214 - loss: 1.1559 - val_accuracy: 0.4167 - val_loss: 1.0861\nEpoch 8/15\n4/4 - 0s - 110ms/step - accuracy: 0.4821 - loss: 1.1633 - val_accuracy: 0.4167 - val_loss: 1.1320\nEpoch 9/15\n4/4 - 0s - 108ms/step - accuracy: 0.4286 - loss: 1.1072 - val_accuracy: 0.4167 - val_loss: 1.1364\nEpoch 10/15\n4/4 - 0s - 102ms/step - accuracy: 0.4464 - loss: 1.2283 - val_accuracy: 0.4167 - val_loss: 1.0608\nEpoch 11/15\n4/4 - 0s - 111ms/step - accuracy: 0.3393 - loss: 1.2058 - val_accuracy: 0.4167 - val_loss: 1.0574\nEpoch 12/15\n4/4 - 0s - 110ms/step - accuracy: 0.3750 - loss: 1.1500 - val_accuracy: 0.4167 - val_loss: 1.0409\nEpoch 13/15\n4/4 - 0s - 112ms/step - accuracy: 0.4464 - loss: 1.0872 - val_accuracy: 0.4167 - val_loss: 1.1145\nEpoch 14/15\n4/4 - 0s - 110ms/step - accuracy: 0.4464 - loss: 1.1678 - val_accuracy: 0.4167 - val_loss: 1.1773\nEpoch 15/15\n4/4 - 0s - 110ms/step - accuracy: 0.4464 - loss: 1.0858 - val_accuracy: 0.4167 - val_loss: 1.1174\n‚úÖ model3 Test Accuracy: 0.5833\n\n===== Training model4 =====\nEpoch 1/15\n4/4 - 7s - 2s/step - accuracy: 0.2679 - loss: 1.9372 - val_accuracy: 0.2500 - val_loss: 1.4064\nEpoch 2/15\n4/4 - 0s - 103ms/step - accuracy: 0.2321 - loss: 1.7530 - val_accuracy: 0.2500 - val_loss: 1.4292\nEpoch 3/15\n4/4 - 0s - 106ms/step - accuracy: 0.3036 - loss: 1.5509 - val_accuracy: 0.2500 - val_loss: 1.3958\nEpoch 4/15\n4/4 - 0s - 108ms/step - accuracy: 0.2857 - loss: 1.6836 - val_accuracy: 0.3333 - val_loss: 1.3666\nEpoch 5/15\n4/4 - 0s - 108ms/step - accuracy: 0.3571 - loss: 1.4243 - val_accuracy: 0.3333 - val_loss: 1.3688\nEpoch 6/15\n4/4 - 0s - 112ms/step - accuracy: 0.1607 - loss: 1.7625 - val_accuracy: 0.3333 - val_loss: 1.3852\nEpoch 7/15\n4/4 - 0s - 100ms/step - accuracy: 0.2679 - loss: 1.6603 - val_accuracy: 0.2500 - val_loss: 1.3765\nEpoch 8/15\n4/4 - 0s - 99ms/step - accuracy: 0.2500 - loss: 1.5223 - val_accuracy: 0.3333 - val_loss: 1.3532\nEpoch 9/15\n4/4 - 0s - 109ms/step - accuracy: 0.2500 - loss: 1.6745 - val_accuracy: 0.3333 - val_loss: 1.3646\nEpoch 10/15\n4/4 - 0s - 102ms/step - accuracy: 0.3214 - loss: 1.6177 - val_accuracy: 0.2500 - val_loss: 1.3778\nEpoch 11/15\n4/4 - 0s - 97ms/step - accuracy: 0.2500 - loss: 1.5155 - val_accuracy: 0.1667 - val_loss: 1.3921\nEpoch 12/15\n4/4 - 0s - 91ms/step - accuracy: 0.2500 - loss: 1.6439 - val_accuracy: 0.1667 - val_loss: 1.3983\nEpoch 13/15\n4/4 - 0s - 100ms/step - accuracy: 0.2857 - loss: 1.5140 - val_accuracy: 0.2500 - val_loss: 1.3794\n‚úÖ model4 Test Accuracy: 0.1667\n","output_type":"stream"}],"execution_count":8},{"id":"474853f5-2044-41b9-bfdf-ee7abf8e9670","cell_type":"code","source":"# ------------------------------------------------------\n# üß´ 6. Example Prediction\n# ------------------------------------------------------\nexample_seq = \"GCCTGTGTGATGATGGAGCTGGGAATACTCTGGGGAGAGAGTCCTCTTTTCAGCTGTATTTTGCTTCCTTCCCACACAGAC\"\npredict_sequence(example_seq, \"model4_bilstm_transformer.keras\", \"model4_tokenizer.pkl\", \"model4_le.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:33:42.617764Z","iopub.execute_input":"2025-10-26T14:33:42.619100Z","iopub.status.idle":"2025-10-26T14:33:43.258798Z","shell.execute_reply.started":"2025-10-26T14:33:42.619042Z","shell.execute_reply":"2025-10-26T14:33:43.257714Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'OTHERS'"},"metadata":{}}],"execution_count":17},{"id":"215ee649-fc8f-47f4-90a2-f0257db6f06b","cell_type":"code","source":"# =========================================================\n# üîÆ Prediction for All 4 Models\n# =========================================================\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport joblib\n\n# ----------------------------\n# Prediction Helper Function\n# ----------------------------\ndef predict_sequence(sequence, model_path, tokenizer_path, le_path, max_len=200):\n    \"\"\"Load model + tokenizer + encoder and predict GeneType for one DNA sequence.\"\"\"\n    model = load_model(model_path)\n    tokenizer = joblib.load(tokenizer_path)\n    le = joblib.load(le_path)\n\n    seq = pad_sequences(tokenizer.texts_to_sequences([sequence]), maxlen=max_len, padding='post')\n    pred = model.predict(seq, verbose=0)\n\n    # Decode prediction\n    if pred.shape[1] == 1:  # Binary model\n        predicted = int(pred[0][0] > 0.5)\n    else:\n        predicted = np.argmax(pred, axis=1)[0]\n\n    gene_type = le.inverse_transform([predicted])[0]\n    return gene_type\n\n\n# ----------------------------\n# Predict Using All 4 Models\n# ----------------------------\ndef predict_all_models(sequence):\n    results = {}\n\n    for i in range(1, 5):\n        model_path = f\"model{i}_bilstm_transformer.keras\"\n        tokenizer_path = f\"model{i}_tokenizer.pkl\"\n        le_path = f\"model{i}_le.pkl\"\n\n        try:\n            prediction = predict_sequence(sequence, model_path, tokenizer_path, le_path)\n            results[f\"Model {i}\"] = prediction\n        except Exception as e:\n            results[f\"Model {i}\"] = f\"‚ö†Ô∏è Error: {e}\"\n\n    print(\"\\nüß¨ Prediction Summary for Input Sequence:\")\n    print(\"=====================================\")\n    for model, result in results.items():\n        print(f\"{model}: {result}\")\n\n    return results\n\n\n# ----------------------------\n# üî¨ Example Usage\n# ----------------------------\nexample_seq = \"GGGACAGGGGGC\"\nresults = predict_all_models(example_seq)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T14:34:34.899157Z","iopub.execute_input":"2025-10-26T14:34:34.899504Z","iopub.status.idle":"2025-10-26T14:34:37.449749Z","shell.execute_reply.started":"2025-10-26T14:34:34.899485Z","shell.execute_reply":"2025-10-26T14:34:37.448268Z"}},"outputs":[{"name":"stdout","text":"\nüß¨ Prediction Summary for Input Sequence:\n=====================================\nModel 1: BIOLOGICAL_REGION\nModel 2: OTHERS\nModel 3: OTHERS\nModel 4: BIOLOGICAL_REGION\n","output_type":"stream"}],"execution_count":20},{"id":"536bfb2a-4eeb-4d4c-98fb-a3ee7e7ee005","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}