{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12406681,"sourceType":"datasetVersion","datasetId":7823994}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (Conv1D, MaxPooling1D, Flatten, Dense, Dropout, \n                                     Embedding, LSTM, Bidirectional, Input, concatenate, \n                                     LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D)\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# -----------------------------\n# Simulated data for demo\n# -----------------------------\nNUM_CLASSES = 5\nMAX_LEN = 1000\nVOCAB_SIZE = 5\n\nnp.random.seed(42)\nX = np.random.randint(0, VOCAB_SIZE, (800, MAX_LEN))\ny_multi = tf.keras.utils.to_categorical(np.random.randint(0, NUM_CLASSES, 800), NUM_CLASSES)\ny_bin1 = np.random.randint(0, 2, 800)\ny_bin2 = np.random.randint(0, 2, 800)\ny_bin3 = np.random.randint(0, 2, 800)\n\nX_train, X_val, y_train_multi, y_val_multi = train_test_split(X, y_multi, test_size=0.2)\n_, _, y_train_bin1, y_val_bin1 = train_test_split(X, y_bin1, test_size=0.2)\n_, _, y_train_bin2, y_val_bin2 = train_test_split(X, y_bin2, test_size=0.2)\n_, _, y_train_bin3, y_val_bin3 = train_test_split(X, y_bin3, test_size=0.2)\n\n# -----------------------------\n# Model 1: 1D CNN (Multiclass)\n# -----------------------------\ncnn_model = Sequential([\n    Embedding(VOCAB_SIZE, 64, input_length=MAX_LEN),\n    Conv1D(128, 5, activation='relu'),\n    MaxPooling1D(2),\n    Conv1D(64, 3, activation='relu'),\n    GlobalAveragePooling1D(),\n    Dense(128, activation='relu'),\n    Dropout(0.4),\n    Dense(NUM_CLASSES, activation='softmax')\n])\ncnn_model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\ncnn_model.fit(X_train, y_train_multi, epochs=5, validation_data=(X_val, y_val_multi), verbose=1)\n\n# -----------------------------\n# Model 2: BiLSTM (Binary)\n# -----------------------------\ninput_seq = Input(shape=(MAX_LEN,))\nx = Embedding(VOCAB_SIZE, 64)(input_seq)\nx = Bidirectional(LSTM(64, return_sequences=False))(x)\nx = Dropout(0.3)(x)\noutput = Dense(1, activation='sigmoid')(x)\nbilstm_model = Model(input_seq, output)\nbilstm_model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\nbilstm_model.fit(X_train, y_train_bin1, epochs=5, validation_data=(X_val, y_val_bin1), verbose=1)\n\n# -----------------------------\n# Model 3: CNN + BiLSTM (Binary)\n# -----------------------------\ninput_seq = Input(shape=(MAX_LEN,))\nx = Embedding(VOCAB_SIZE, 64)(input_seq)\ncnn_out = Conv1D(64, 5, activation='relu')(x)\ncnn_out = MaxPooling1D(2)(cnn_out)\nlstm_out = Bidirectional(LSTM(64))(cnn_out)\ndense = Dense(64, activation='relu')(lstm_out)\noutput = Dense(1, activation='sigmoid')(dense)\ncnn_lstm_model = Model(input_seq, output)\ncnn_lstm_model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\ncnn_lstm_model.fit(X_train, y_train_bin2, epochs=5, validation_data=(X_val, y_val_bin2), verbose=1)\n\n# -----------------------------\n# Model 4: Transformer Encoder (Binary)\n# -----------------------------\ninput_seq = Input(shape=(MAX_LEN,))\nx = Embedding(VOCAB_SIZE, 64)(input_seq)\nattn = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\nx = LayerNormalization()(x + attn)\nx = GlobalAveragePooling1D()(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.3)(x)\noutput = Dense(1, activation='sigmoid')(x)\ntransformer_model = Model(input_seq, output)\ntransformer_model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\ntransformer_model.fit(X_train, y_train_bin3, epochs=5, validation_data=(X_val, y_val_bin3), verbose=1)\n\n# -----------------------------\n# Evaluation\n# -----------------------------\nmodels = {\n    \"CNN\": cnn_model,\n    \"BiLSTM\": bilstm_model,\n    \"CNN+BiLSTM\": cnn_lstm_model,\n    \"Transformer\": transformer_model\n}\n\nprint(\"\\n=== Model Performance Summary ===\")\nfor name, model in models.items():\n    if name == \"CNN\":\n        val_pred = np.argmax(model.predict(X_val), axis=1)\n        true = np.argmax(y_val_multi, axis=1)\n    else:\n        val_pred = (model.predict(X_val) > 0.5).astype(int).flatten()\n        true = [y_val_bin1, y_val_bin2, y_val_bin3][[\"BiLSTM\",\"CNN+BiLSTM\",\"Transformer\"].index(name)]\n    acc = accuracy_score(true, val_pred)\n    print(f\"{name:<15} Accuracy: {acc*100:.2f}%\")\n\n# -----------------------------\n# Ensemble Voting (Weighted)\n# -----------------------------\npred_cnn = np.argmax(cnn_model.predict(X_val), axis=1)\npred_bilstm = (bilstm_model.predict(X_val) > 0.5).astype(int).flatten()\npred_cnn_lstm = (cnn_lstm_model.predict(X_val) > 0.5).astype(int).flatten()\npred_trans = (transformer_model.predict(X_val) > 0.5).astype(int).flatten()\n\n# Combine binary votes (majority)\nensemble_votes = np.round((pred_bilstm + pred_cnn_lstm + pred_trans) / 3)\nensemble_acc = accuracy_score(y_val_bin3, ensemble_votes)\nprint(f\"\\nâœ… Ensemble Binary Accuracy (final stage): {ensemble_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:49:17.534210Z","iopub.execute_input":"2025-10-28T16:49:17.534609Z","iopub.status.idle":"2025-10-28T16:56:32.009137Z","shell.execute_reply.started":"2025-10-28T16:49:17.534584Z","shell.execute_reply":"2025-10-28T16:56:32.008258Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n2025-10-28 16:49:17.827880: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 189ms/step - accuracy: 0.2110 - loss: 1.6092 - val_accuracy: 0.2188 - val_loss: 1.6089\nEpoch 2/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - accuracy: 0.1722 - loss: 1.6105 - val_accuracy: 0.2313 - val_loss: 1.6085\nEpoch 3/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - accuracy: 0.2186 - loss: 1.6090 - val_accuracy: 0.2313 - val_loss: 1.6080\nEpoch 4/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.2090 - loss: 1.6096 - val_accuracy: 0.2188 - val_loss: 1.6078\nEpoch 5/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.1948 - loss: 1.6097 - val_accuracy: 0.2188 - val_loss: 1.6078\nEpoch 1/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 613ms/step - accuracy: 0.5306 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\nEpoch 2/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 561ms/step - accuracy: 0.4484 - loss: 0.6939 - val_accuracy: 0.4938 - val_loss: 0.6931\nEpoch 3/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 591ms/step - accuracy: 0.5096 - loss: 0.6930 - val_accuracy: 0.5125 - val_loss: 0.6930\nEpoch 4/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 562ms/step - accuracy: 0.5275 - loss: 0.6920 - val_accuracy: 0.5125 - val_loss: 0.6929\nEpoch 5/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 562ms/step - accuracy: 0.5436 - loss: 0.6923 - val_accuracy: 0.5125 - val_loss: 0.6929\nEpoch 1/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 376ms/step - accuracy: 0.4702 - loss: 0.6934 - val_accuracy: 0.5625 - val_loss: 0.6929\nEpoch 2/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 319ms/step - accuracy: 0.5004 - loss: 0.6930 - val_accuracy: 0.5562 - val_loss: 0.6927\nEpoch 3/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 328ms/step - accuracy: 0.5578 - loss: 0.6928 - val_accuracy: 0.5750 - val_loss: 0.6926\nEpoch 4/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 323ms/step - accuracy: 0.5295 - loss: 0.6927 - val_accuracy: 0.5625 - val_loss: 0.6924\nEpoch 5/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 329ms/step - accuracy: 0.5641 - loss: 0.6926 - val_accuracy: 0.5375 - val_loss: 0.6929\nEpoch 1/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.4874 - loss: 0.7039 - val_accuracy: 0.5188 - val_loss: 0.6925\nEpoch 2/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.5122 - loss: 0.6947 - val_accuracy: 0.5000 - val_loss: 0.6930\nEpoch 3/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.5116 - loss: 0.6896 - val_accuracy: 0.5000 - val_loss: 0.6936\nEpoch 4/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.4794 - loss: 0.7057 - val_accuracy: 0.5000 - val_loss: 0.6974\nEpoch 5/5\n\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.5469 - loss: 0.6921 - val_accuracy: 0.5000 - val_loss: 0.7006\n\n=== Model Performance Summary ===\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\nCNN             Accuracy: 21.88%\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step\nBiLSTM          Accuracy: 51.25%\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step\nCNN+BiLSTM      Accuracy: 53.75%\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 954ms/step\nTransformer     Accuracy: 50.00%\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 941ms/step\n\nâœ… Ensemble Binary Accuracy (final stage): 51.25%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport tensorflow as tf\n\ndef save_trained_models(models_dict, save_dir=\"trained_models\"):\n    \"\"\"\n    Saves all trained models to disk.\n    Args:\n        models_dict: dict of {'model_name': model_object}\n        save_dir: target directory to store .h5 files\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    for name, model in models_dict.items():\n        path = os.path.join(save_dir, f\"{name}.h5\")\n        model.save(path)\n        print(f\"âœ… Saved {name} â†’ {path}\")\n    print(\"\\nAll models saved successfully!\")\n\n# Example usage (after training all 4 models)\nmodels_dict = {\n    \"model1_cnn_multiclass\": cnn_model,\n    \"model2_bilstm_binary\": bilstm_model,\n    \"model3_cnn_bilstm_binary\": cnn_lstm_model,\n    \"model4_transformer_binary\": transformer_model\n}\n\nsave_trained_models(models_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:13:38.247399Z","iopub.execute_input":"2025-10-28T17:13:38.247757Z","iopub.status.idle":"2025-10-28T17:13:38.404962Z","shell.execute_reply.started":"2025-10-28T17:13:38.247731Z","shell.execute_reply":"2025-10-28T17:13:38.404024Z"}},"outputs":[{"name":"stdout","text":"âœ… Saved model1_cnn_multiclass â†’ trained_models/model1_cnn_multiclass.h5\nâœ… Saved model2_bilstm_binary â†’ trained_models/model2_bilstm_binary.h5\nâœ… Saved model3_cnn_bilstm_binary â†’ trained_models/model3_cnn_bilstm_binary.h5\nâœ… Saved model4_transformer_binary â†’ trained_models/model4_transformer_binary.h5\n\nAll models saved successfully!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense, Dropout\n\n# ======================================\n# ğŸ§© STEP 2: Custom Transformer Block\n# (needed to load transformer model)\n# ======================================\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super(TransformerBlock, self).__init__(**kwargs)\n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            Dense(ff_dim, activation=\"relu\"),\n            Dense(embed_dim)\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n# ======================================\n# ğŸ§¬ STEP 3: Load Saved Models\n# ======================================\ndef load_saved_models(save_dir=\"trained_models\"):\n    model_files = {\n        \"cnn_multiclass\": \"model1_cnn_multiclass.h5\",\n        \"bilstm_binary\": \"model2_bilstm_binary.h5\",\n        \"cnn_bilstm_binary\": \"model3_cnn_bilstm_binary.h5\",\n        \"transformer_binary\": \"model4_transformer_binary.h5\"\n    }\n\n    models = {}\n    for name, file in model_files.items():\n        path = os.path.join(save_dir, file)\n        if not os.path.exists(path):\n            print(f\"âŒ Model not found: {path}\")\n            continue\n\n        try:\n            if \"transformer\" in name:\n                models[name] = tf.keras.models.load_model(\n                    path,\n                    custom_objects={\n                        \"TransformerBlock\": TransformerBlock,\n                        \"MultiHeadAttention\": tf.keras.layers.MultiHeadAttention,\n                        \"LayerNormalization\": tf.keras.layers.LayerNormalization,\n                        \"Dropout\": tf.keras.layers.Dropout,\n                        \"Dense\": tf.keras.layers.Dense\n                    },\n                    compile=False\n                )\n            else:\n                models[name] = tf.keras.models.load_model(path)\n\n            print(f\"âœ… Loaded {name.upper()} â†’ {path}\")\n\n        except Exception as e:\n            print(f\"âš ï¸ Error loading {name.upper()}: {e}\")\n\n    return models\n\n# ======================================\n# ğŸ”¡ STEP 4: DNA Sequence Preprocessing\n# ======================================\ndef preprocess_dna(seq, maxlen=1000):\n    seq = seq.upper().strip()\n    mapping = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    encoded = [mapping.get(ch, 0) for ch in seq]\n    if len(encoded) < maxlen:\n        encoded += [0] * (maxlen - len(encoded))\n    else:\n        encoded = encoded[:maxlen]\n    return np.array(encoded).reshape(1, maxlen)\n\n# ======================================\n# ğŸ§¬ STEP 5: Predict Gene Type\n# ======================================\ndef predict_gene_type(sequence, models):\n    x = preprocess_dna(sequence)\n    results = {}\n\n    # Label names for CNN multiclass output\n    gene_labels = [\"PSEUDO\", \"BIOLOGICAL_REGION\", \"ncRNA\", \"OTHER\"]\n\n    # --- CNN Multiclass ---\n    if \"cnn_multiclass\" in models:\n        pred_cnn = models[\"cnn_multiclass\"].predict(x, verbose=0)\n        pred_class = int(np.argmax(pred_cnn))\n        results[\"CNN_Multiclass\"] = {\n            \"Predicted_Gene_Type\": gene_labels[pred_class],\n            \"Confidence\": float(np.max(pred_cnn))\n        }\n\n    # --- Binary Models ---\n    binary_mapping = {\n        \"bilstm_binary\": \"PSEUDO\",\n        \"cnn_bilstm_binary\": \"BIOLOGICAL_REGION\",\n        \"transformer_binary\": \"ncRNA\"\n    }\n\n    for key, label in binary_mapping.items():\n        if key in models:\n            pred = models[key].predict(x, verbose=0).flatten()[0]\n            results[key.upper()] = {\n                \"Predicted_Gene_Type\": label if pred > 0.5 else \"OTHER\",\n                \"Confidence\": round(float(pred), 3)\n            }\n\n    return results\n\n# ======================================\n# ğŸš€ STEP 6: Run Prediction Example\n# ======================================\nmodels = load_saved_models()\n\ndna_seq = input(\"\\nğŸ§¬ Enter DNA Sequence: \").strip()\npredictions = predict_gene_type(dna_seq, models)\n\nprint(\"\\n========== ğŸ§¬ PREDICTION RESULTS ==========\")\nfor model_name, info in predictions.items():\n    print(f\"{model_name}: {info}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:47:56.249552Z","iopub.execute_input":"2025-10-28T17:47:56.250011Z","iopub.status.idle":"2025-10-28T17:48:04.078656Z","shell.execute_reply.started":"2025-10-28T17:47:56.249980Z","shell.execute_reply":"2025-10-28T17:48:04.077359Z"}},"outputs":[{"name":"stdout","text":"âœ… Loaded CNN_MULTICLASS â†’ trained_models/model1_cnn_multiclass.h5\nâœ… Loaded BILSTM_BINARY â†’ trained_models/model2_bilstm_binary.h5\nâœ… Loaded CNN_BILSTM_BINARY â†’ trained_models/model3_cnn_bilstm_binary.h5\nâš ï¸ Error loading TRANSFORMER_BINARY: too many positional arguments\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nğŸ§¬ Enter DNA Sequence:  TTGTGATTATGTTATTTTACATGGCAAAAGGGACTTTGCAGCGGTGACTAACATAAGGACCTTGAGATGAGGAGATTATCCCCAGGAACATAATCACATGAGTTCTTAAACACAGGGGACATTTGCTAATGATGGCCCAAGGGAACTGACTATAGAAAAATGGAAAGAGAGTTGCTTATCAGCGTAAGAAACTTTTGGCCTGAGACGATGGGGTTTTCTAAATATAGGATCATGTAATCCACAAACAGAGATAGTTTGACTTCCTCTCTTCCTATTTGAATACACTTTATTTCTT\n"},{"name":"stdout","text":"\n========== ğŸ§¬ PREDICTION RESULTS ==========\nCNN_Multiclass: {'Predicted_Gene_Type': 'PSEUDO', 'Confidence': 0.20418643951416016}\nBILSTM_BINARY: {'Predicted_Gene_Type': 'PSEUDO', 'Confidence': 0.5}\nCNN_BILSTM_BINARY: {'Predicted_Gene_Type': 'OTHER', 'Confidence': 0.498}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}